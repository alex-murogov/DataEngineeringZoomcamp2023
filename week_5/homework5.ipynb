{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c84b712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "import warnings\n",
    "from os.path import exists\n",
    "\n",
    "import os.path\n",
    "\n",
    "from pyspark.sql.functions import udf, max, desc\n",
    "# from pyspark.sql.types import DoubleType\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81e1eaf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/26 14:55:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Week5\").config('spark.executor.instances', 4).getOrCreate()\n",
    "# spark.conf.set(\"spark.executor.memory\", \"10g\")\n",
    "# spark.conf.set(\"spark.executor.cores\", \"4\")\n",
    "\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b3b127",
   "metadata": {},
   "source": [
    "## Question 1:\n",
    "Install Spark and PySpark\n",
    "\n",
    "Install Spark\n",
    "Run PySpark\n",
    "Create a local spark session\n",
    "Execute spark.version.\n",
    "What's the output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c99daaee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da28509b",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f4e278",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "HVFHW June 2021\n",
    "\n",
    "Read it with Spark using the same schema as we did in the lessons.\n",
    "We will use this dataset for all the remaining questions.\n",
    "Repartition it to 12 partitions and save it to parquet.\n",
    "What is the average size of the Parquet (ending with .parquet extension) Files that were created (in MB)? Select the answer which most closely matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d71815",
   "metadata": {},
   "outputs": [],
   "source": [
    "#download file and store locally as CSV\n",
    "# https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhvhv/fhvhv_tripdata_2021-06.csv.gz\n",
    "\n",
    "month = 6\n",
    "year = 2021\n",
    "dataset_file = f'fhvhv_tripdata_{year}-{month:02}'\n",
    "\n",
    "#check if fifle already exists\n",
    "file_exists = os.path.exists(f'{dataset_file}.csv')\n",
    "\n",
    "if not file_exists:\n",
    "    dataset_url = f'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhvhv/{dataset_file}.csv.gz'\n",
    "    df_pandas = pd.read_csv(dataset_url, compression='gzip')\n",
    "    df_pandas.to_csv(f'{dataset_file}.csv')\n",
    "else:\n",
    "    print('file already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc355340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read it with Spark using the same schema as we did in the lessons.\n",
    "\n",
    "q2_schema = StructType(\n",
    "    [StructField('_c0', StringType(), True), \n",
    "     StructField('dispatching_base_num', StringType(), True),\n",
    "     StructField('pickup_datetime', TimestampType(), True),\n",
    "     StructField('dropOff_datetime', TimestampType(), True),\n",
    "     StructField('PUlocationID', FloatType(), True), \n",
    "     StructField('DOlocationID', FloatType(), True),\n",
    "     StructField('SR_Flag', StringType(), True), \n",
    "     StructField('Affiliated_base_number', StringType(), True)])\n",
    "\n",
    "df_q2 = (spark.read\n",
    "         .option(\"header\", \"true\")\n",
    "         .schema(q2_schema)\n",
    "         .csv('fhvhv_tripdata_2021-06.csv')\n",
    "         .drop(F.col(\"_c0\"))\n",
    "        )\n",
    "\n",
    "df_q2 = (df_q2\n",
    "         .withColumn(\"PUlocationID\", df_q2[\"PUlocationID\"].cast(IntegerType()))\n",
    "         .withColumn(\"DOlocationID\", df_q2[\"DOlocationID\"].cast(IntegerType()))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0aa78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_q2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1289bdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repartition it to 12 partitions and save it to parquet.\n",
    "df_q2 = df_q2.repartition(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0281118b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_q2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0428c52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_q2.write.parquet('data/fhvhv/', mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f3373c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the average size of the Parquet (ending with .parquet extension) Files that were created (in MB)?\n",
    "# 23,5Mb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3131a7",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230e36d3",
   "metadata": {},
   "source": [
    "## Question 3:\n",
    "Count records\n",
    "\n",
    "How many taxi trips were there on June 15?\n",
    "\n",
    "Consider only trips that started on June 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f742d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_q2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c06327a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_q3 = (df_q2\n",
    "         .withColumn(\"pickup_date\", F.to_date(df_q2.pickup_datetime))\n",
    "        .withColumn(\"dropOff_date\", F.to_date(df_q2.dropOff_datetime))\n",
    "        .filter(\"pickup_date = '2021-06-15'\") \n",
    "        )\n",
    "\n",
    "df_q3.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b14d3be",
   "metadata": {},
   "source": [
    "________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0552a1",
   "metadata": {},
   "source": [
    "## Question 4:\n",
    "Longest trip for each day\n",
    "\n",
    "Now calculate the duration for each trip.\n",
    "How long was the longest trip in Hours?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4224d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_q4 = spark.read.parquet('data/fhvhv/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62ed5d3",
   "metadata": {},
   "source": [
    "### spark method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37827f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_q4 = df_q4.withColumn('DiffInSeconds', F.col(\"dropOff_datetime\").cast(\"long\") - F.col('pickup_datetime').cast(\"long\")) \\\n",
    ".withColumn('DiffInMinutes',(F.col('DiffInSeconds')/60))\\\n",
    ".withColumn('DiffInHours',(F.col('DiffInSeconds')/3600))\\\n",
    ".select('pickup_datetime', 'dropOff_datetime', 'DiffInHours', 'DiffInSeconds', 'DiffInMinutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f813ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = df_q4.selectExpr('max(DiffInHours) as max_value').first().max_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897a862a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Maximum hours_between: \", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83780d8d",
   "metadata": {},
   "source": [
    "### sparksql method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487372f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_q4.createOrReplaceTempView('fhvhv_data')\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    pickup_datetime,\n",
    "    dropoff_datetime,\n",
    "    DATEDIFF(hour, pickup_datetime, dropoff_datetime) AS DateDiff\n",
    "FROM\n",
    "    fhvhv_data\n",
    "ORDER BY\n",
    "    DateDiff DESC\n",
    "\"\"\").show(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a96f7ec",
   "metadata": {},
   "source": [
    "______"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaffd89a",
   "metadata": {},
   "source": [
    "## Question 5:\n",
    "User Interface\n",
    "\n",
    "Sparkâ€™s User Interface which shows application's dashboard runs on which local port?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d2d7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "http://localhost:4040/jobs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756d2ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2930976b",
   "metadata": {},
   "source": [
    "__________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f23389",
   "metadata": {},
   "source": [
    "## Question 6:\n",
    "Most frequent pickup location zone\n",
    "\n",
    "Load the zone lookup data into a temp view in Spark\n",
    "Zone Data\n",
    "\n",
    "Using the zone lookup data and the fhvhv June 2021 data, what is the name of the most frequent pickup location zone?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ec6f489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LocationID', 'Borough', 'Zone', 'service_zone']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_zones = spark.read.format('csv').option('header','true').load('zone_lookup.csv')\n",
    "\n",
    "df_zones.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2bdb40c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dispatching_base_num',\n",
       " 'pickup_datetime',\n",
       " 'dropOff_datetime',\n",
       " 'PUlocationID',\n",
       " 'DOlocationID',\n",
       " 'SR_Flag',\n",
       " 'Affiliated_base_number']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_q6= spark.read.parquet('data/fhvhv/')\n",
    "\n",
    "df_q6.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6dd631",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c2484bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dispatching_base_num',\n",
       " 'pickup_datetime',\n",
       " 'dropOff_datetime',\n",
       " 'PUlocationID',\n",
       " 'DOlocationID',\n",
       " 'SR_Flag',\n",
       " 'Affiliated_base_number',\n",
       " 'LocationID',\n",
       " 'Borough',\n",
       " 'Zone',\n",
       " 'service_zone']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_join = df_q6.join(df_zones, df_q6.PUlocationID == df_zones.LocationID)\n",
    "\n",
    "df_join.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0442445",
   "metadata": {},
   "source": [
    "### Spark method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cf54fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "most_frequent_zone = df_join.groupBy('Zone') \\\n",
    "                      .count() \\\n",
    "                      .orderBy(desc('count')) \\\n",
    "                      .first()['Zone']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ac5d3c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Crown Heights North'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_frequent_zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617d7f2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdc896dc",
   "metadata": {},
   "source": [
    "### SparkSQL method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffe94324",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones.registerTempTable('zones')\n",
    "df_q6.registerTempTable('fhvhv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6381ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd9adfda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:======================================>                  (8 + 4) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+\n",
      "|               zone|number_records|\n",
      "+-------------------+--------------+\n",
      "|Crown Heights North|        231279|\n",
      "+-------------------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    z.zone AS zone,\n",
    "    COUNT(1) AS number_records\n",
    "FROM fhvhv as tr\n",
    "INNER JOIN zones as z on z.LocationID = tr.PUlocationID\n",
    "GROUP BY 1\n",
    "ORDER BY COUNT(1) DESC\n",
    "LIMIT 1 \n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e195e12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
